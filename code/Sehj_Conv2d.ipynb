{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from dataloader import DataLoader\n",
    "import numpy as np\n",
    "import utils\n",
    "# from torchsummary import summary\n",
    "import os \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(utils.train_dir, batch_size=24, transfer=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available!\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu available!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"gpu NOT available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'011', '016', '023', '030'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(utils.val_dir)\n",
    "print('val')\n",
    "set([fname.split('reg')[1].split('_')[0] for fname in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'005', '006', '017', '019'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(utils.test_dir)\n",
    "print('test')\n",
    "set([fname.split('reg')[1].split('_')[0] for fname in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "completed: {'004', '027', '012', '008', '024', '014', '020', '007', '034', '015'}\n",
      "remaining: set()\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(utils.train_dir)\n",
    "print('train')\n",
    "completed = set([fname.split('reg')[1].split('_')[0] for fname in files])\n",
    "print('completed:', completed)\n",
    "remaining = set(['015','004','014','024','020','007','008','027','034','012']) - completed\n",
    "print('remaining:', remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13069"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(utils.train_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validating and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_accuracy(model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    \n",
    "    loader = DataLoader(utils.train_dir, batch_size=10, transfer=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (fxy, x, y) in enumerate(loader):\n",
    "            \n",
    "            x = torch.from_numpy(x)\n",
    "            y = torch.from_numpy(y)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            if i > 5:\n",
    "                break\n",
    "            \n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val_accuracy(model, it):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    \n",
    "    loader = DataLoader(utils.val_dir, batch_size=24, transfer=False)\n",
    "    \n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (fx, x, y) in enumerate(loader):\n",
    "            total += 1\n",
    "            x = torch.from_numpy(x)\n",
    "            y = torch.from_numpy(y)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            val_loss += F.cross_entropy(scores, y)\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            acc = float(num_correct) / num_samples\n",
    "            print('Val accuracy: %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "            print('Val loss: {0:0.4f}'.format(val_loss / total))\n",
    "\n",
    "    return val_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "    \n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = torchvision.models.vgg19(pretrained=False)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same structure as vgg19 model print(torchvision.models.vgg19(pretrained=False, progress = True))\n",
    "model = nn.Sequential(\n",
    "    \n",
    "    nn.Conv2d(75, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), #64\n",
    "#     nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    \n",
    "    nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    \n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    \n",
    "    nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    \n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(in_features= 4608, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    \n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    \n",
    "    nn.Linear(in_features=4096, out_features=2, bias=True),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DEBUG MODEL:::\n",
    "# add PrintLayer(), or use summary from torchsummary module:\n",
    "# summary(your_model, input_size=(channels, H, W))\n",
    "# https://towardsdatascience.com/model-summary-in-pytorch-b5a1e4b64d25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitLoader = DataLoader(utils.train_dir, batch_size=24, transfer=False, mode = 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_overfit(model, optimizer, epochs=10):\n",
    "#     \"\"\"\n",
    "#     Train a model on image data using the PyTorch Module API.\n",
    "    \n",
    "#     Inputs:\n",
    "#     - model: A PyTorch Module giving the model to train.\n",
    "#     - optimizer: An Optimizer object we will use to train the model\n",
    "#     - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "#     Returns: Nothing, but prints model accuracies during training.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print_every = 4\n",
    "#     train_every = 10\n",
    "#     val_every = 20\n",
    "#     model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "#     loss_history = []\n",
    "#     for e in range(epochs):\n",
    "#         for t, (fxy, x, y) in enumerate(overfitLoader):\n",
    "            \n",
    "#             model.train()  # put model to training mode\n",
    "            \n",
    "#             x = torch.from_numpy(x)\n",
    "#             y = torch.from_numpy(y)\n",
    "            \n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "#             scores = model(x)\n",
    "#             loss = F.cross_entropy(scores, y)\n",
    "\n",
    "#             # Zero out all of the gradients for the variables which the optimizer\n",
    "#             # will update.\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # This is the backwards pass: compute the gradient of the loss with\n",
    "#             # respect to each  parameter of the model.\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Actually update the parameters of the model using the gradients\n",
    "#             # computed by the backwards pass.\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if t % print_every == 0:\n",
    "#                 print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            \n",
    "#             loss_history.append(loss.item())\n",
    "    \n",
    "#     return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.5847\n",
      "Iteration 4, loss = 0.3507\n",
      "Iteration 8, loss = 0.3354\n",
      "Iteration 12, loss = 0.3514\n",
      "Iteration 16, loss = 0.1937\n",
      "Iteration 20, loss = 0.1278\n",
      "Iteration 24, loss = 0.5615\n",
      "Iteration 28, loss = 0.2393\n",
      "Iteration 32, loss = 0.3077\n",
      "Iteration 36, loss = 0.3151\n",
      "Iteration 40, loss = 0.1274\n",
      "Iteration 44, loss = 0.1092\n",
      "Iteration 48, loss = 0.3363\n",
      "Iteration 52, loss = 0.1074\n",
      "Iteration 56, loss = 0.0847\n",
      "Iteration 0, loss = 0.2859\n",
      "Iteration 4, loss = 0.1880\n",
      "Iteration 8, loss = 0.1270\n",
      "Iteration 12, loss = 0.1063\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 1e-5\n",
    "# optimizer = optim.Adam(model.parameters(),\n",
    "#                       lr=learning_rate)\n",
    "\n",
    "# train_overfit(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_histrory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a model on image data using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = DataLoader(utils.train_dir, batch_size=36, transfer=False, normalize=True)\n",
    "#     val_loader = DataLoader(utils.val_dir, batch_size=24, transfer=False)\n",
    "    \n",
    "    print_every = 10\n",
    "    train_every = 10\n",
    "    val_every = 20\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    loss_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    cur_val = 999\n",
    "    consec_increases = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        t = 0\n",
    "        for fdl_train in train_loader:\n",
    "            \n",
    "            fxy, x, y = fdl_train\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            x = torch.from_numpy(x)\n",
    "            y = torch.from_numpy(y)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "#             # validation loss\n",
    "#             with torch.no_grad():\n",
    "#                 model.eval()\n",
    "#                 fxy, x, y = fdl_val\n",
    "#                 x = torch.from_numpy(x)\n",
    "#                 y = torch.from_numpy(y)\n",
    "\n",
    "#                 x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#                 y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "#                 scores = model(x)\n",
    "#                 val_loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iter %d, train loss = %.4f' % (t + print_every, loss.item()))\n",
    "            \n",
    "#             val_history.append(val_loss.item())\n",
    "            \n",
    "#             if val_loss.item() > cur_val:\n",
    "#                 consec_increases += 1\n",
    "#             else:\n",
    "#                 consec_increases = 0\n",
    "            \n",
    "#             cur_val = val_loss.item()\n",
    "            \n",
    "#             if consec_increases >= 3:\n",
    "#                 print('Stopping early due to validation loss increase')\n",
    "#                 torch.save(model, utils.model_dir + \"conv2d_v2_earlystop_epoch{}.pt\".format(e))\n",
    "#                 return loss_history, val_history\n",
    "            \n",
    "            loss_history.append(loss.item())\n",
    "            t += 1\n",
    "            \n",
    "        torch.save(model, utils.model_dir + \"conv2d_v2_std_epoch%s.pt\" % e)\n",
    "    \n",
    "    return loss_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10, train loss = 0.6975\n",
      "Iter 20, train loss = 0.6932\n",
      "Iter 30, train loss = 0.6914\n",
      "Iter 40, train loss = 0.6956\n",
      "Iter 50, train loss = 0.6927\n",
      "Iter 60, train loss = 0.6912\n",
      "Iter 70, train loss = 0.6864\n",
      "Iter 80, train loss = 0.6979\n",
      "Iter 90, train loss = 0.6949\n",
      "Iter 100, train loss = 0.6841\n",
      "Iter 110, train loss = 0.6872\n",
      "Iter 120, train loss = 0.6837\n",
      "Iter 130, train loss = 0.6919\n",
      "Iter 140, train loss = 0.6895\n",
      "Iter 150, train loss = 0.6888\n",
      "Iter 160, train loss = 0.7006\n",
      "Iter 170, train loss = 0.7019\n",
      "Iter 180, train loss = 0.6885\n",
      "Iter 190, train loss = 0.7015\n",
      "Iter 200, train loss = 0.6844\n",
      "Iter 210, train loss = 0.6796\n",
      "Iter 220, train loss = 0.7066\n",
      "Iter 230, train loss = 0.6971\n",
      "Iter 240, train loss = 0.6880\n",
      "Iter 250, train loss = 0.6943\n",
      "Iter 260, train loss = 0.6937\n",
      "Iter 270, train loss = 0.6781\n",
      "Iter 280, train loss = 0.6985\n",
      "Iter 290, train loss = 0.6912\n",
      "Iter 300, train loss = 0.7024\n",
      "Iter 310, train loss = 0.6765\n",
      "Iter 320, train loss = 0.6636\n",
      "Iter 330, train loss = 0.6482\n",
      "Iter 340, train loss = 0.7338\n",
      "Iter 350, train loss = 0.6857\n",
      "Iter 360, train loss = 0.6711\n",
      "Iter 370, train loss = 0.4453\n",
      "Iter 10, train loss = 0.8688\n",
      "Iter 20, train loss = 0.6497\n",
      "Iter 30, train loss = 0.6521\n",
      "Iter 40, train loss = 0.6276\n",
      "Iter 50, train loss = 0.6211\n",
      "Iter 60, train loss = 0.5635\n",
      "Iter 70, train loss = 0.6392\n",
      "Iter 80, train loss = 0.6913\n",
      "Iter 90, train loss = 0.5397\n",
      "Iter 100, train loss = 0.4389\n",
      "Iter 110, train loss = 0.4691\n",
      "Iter 120, train loss = 0.5651\n",
      "Iter 130, train loss = 0.3493\n",
      "Iter 140, train loss = 0.6824\n",
      "Iter 150, train loss = 0.5921\n",
      "Iter 160, train loss = 0.6153\n",
      "Iter 170, train loss = 0.4417\n",
      "Iter 180, train loss = 0.5030\n",
      "Iter 190, train loss = 0.6334\n",
      "Iter 200, train loss = 0.5581\n",
      "Iter 210, train loss = 0.4410\n",
      "Iter 220, train loss = 0.4712\n",
      "Iter 230, train loss = 0.3836\n",
      "Iter 240, train loss = 0.4346\n",
      "Iter 250, train loss = 0.5288\n",
      "Iter 260, train loss = 0.5920\n",
      "Iter 270, train loss = 0.4594\n",
      "Iter 280, train loss = 0.4631\n",
      "Iter 290, train loss = 0.4871\n",
      "Iter 300, train loss = 0.5499\n",
      "Iter 310, train loss = 0.4410\n",
      "Iter 320, train loss = 0.5101\n",
      "Iter 330, train loss = 0.5178\n",
      "Iter 340, train loss = 0.6828\n",
      "Iter 350, train loss = 0.6390\n",
      "Iter 360, train loss = 0.5077\n",
      "Iter 370, train loss = 0.4336\n",
      "Iter 10, train loss = 0.4268\n",
      "Iter 20, train loss = 0.6515\n",
      "Iter 30, train loss = 0.5375\n",
      "Iter 40, train loss = 0.5343\n",
      "Iter 50, train loss = 0.5288\n",
      "Iter 60, train loss = 0.3228\n",
      "Iter 70, train loss = 0.4355\n",
      "Iter 80, train loss = 0.5245\n",
      "Iter 90, train loss = 0.4217\n",
      "Iter 100, train loss = 0.3128\n",
      "Iter 110, train loss = 0.4874\n",
      "Iter 120, train loss = 0.3635\n",
      "Iter 130, train loss = 0.2359\n",
      "Iter 140, train loss = 0.7076\n",
      "Iter 150, train loss = 0.4464\n",
      "Iter 190, train loss = 0.5680\n",
      "Iter 200, train loss = 0.4982\n",
      "Iter 210, train loss = 0.4741\n",
      "Iter 220, train loss = 0.2602\n",
      "Iter 230, train loss = 0.2404\n",
      "Iter 240, train loss = 0.3790\n",
      "Iter 250, train loss = 0.3814\n",
      "Iter 260, train loss = 0.6173\n",
      "Iter 270, train loss = 0.3867\n",
      "Iter 280, train loss = 0.3344\n",
      "Iter 290, train loss = 0.4130\n",
      "Iter 300, train loss = 0.4724\n",
      "Iter 310, train loss = 0.3221\n",
      "Iter 320, train loss = 0.5511\n",
      "Iter 330, train loss = 0.4625\n",
      "Iter 340, train loss = 0.4606\n",
      "Iter 350, train loss = 0.3216\n",
      "Iter 360, train loss = 0.3960\n",
      "Iter 370, train loss = 0.3587\n",
      "Iter 10, train loss = 0.2689\n",
      "Iter 20, train loss = 0.5745\n",
      "Iter 30, train loss = 0.4017\n",
      "Iter 40, train loss = 0.3576\n",
      "Iter 50, train loss = 0.5356\n",
      "Iter 60, train loss = 0.1955\n",
      "Iter 70, train loss = 0.2565\n",
      "Iter 80, train loss = 0.3863\n",
      "Iter 90, train loss = 0.3250\n",
      "Iter 100, train loss = 0.1423\n",
      "Iter 110, train loss = 0.4379\n",
      "Iter 120, train loss = 0.2530\n",
      "Iter 130, train loss = 0.2053\n",
      "Iter 140, train loss = 0.4038\n",
      "Iter 150, train loss = 0.4076\n",
      "Iter 160, train loss = 0.3960\n",
      "Iter 170, train loss = 0.3512\n",
      "Iter 180, train loss = 0.3199\n",
      "Iter 190, train loss = 0.4139\n",
      "Iter 200, train loss = 0.2365\n",
      "Iter 210, train loss = 0.5925\n",
      "Iter 220, train loss = 0.2571\n",
      "Iter 230, train loss = 0.1702\n",
      "Iter 240, train loss = 0.3419\n",
      "Iter 250, train loss = 0.2335\n",
      "Iter 260, train loss = 0.3581\n",
      "Iter 270, train loss = 0.2634\n",
      "Iter 280, train loss = 0.1393\n",
      "Iter 290, train loss = 0.3674\n",
      "Iter 300, train loss = 0.3263\n",
      "Iter 310, train loss = 0.2040\n",
      "Iter 320, train loss = 0.4262\n",
      "Iter 330, train loss = 0.3322\n",
      "Iter 340, train loss = 0.4463\n",
      "Iter 350, train loss = 0.2673\n",
      "Iter 360, train loss = 0.3821\n",
      "Iter 370, train loss = 0.3297\n",
      "Iter 10, train loss = 0.2045\n",
      "Iter 20, train loss = 0.3938\n",
      "Iter 30, train loss = 0.2598\n",
      "Iter 40, train loss = 0.2258\n",
      "Iter 50, train loss = 0.4185\n",
      "Iter 60, train loss = 0.1501\n",
      "Iter 70, train loss = 0.2068\n",
      "Iter 80, train loss = 0.2589\n",
      "Iter 90, train loss = 0.2881\n",
      "Iter 100, train loss = 0.1420\n",
      "Iter 110, train loss = 0.3518\n",
      "Iter 120, train loss = 0.1316\n",
      "Iter 130, train loss = 0.1732\n",
      "Iter 140, train loss = 0.2525\n",
      "Iter 150, train loss = 0.3569\n",
      "Iter 160, train loss = 0.2449\n",
      "Iter 170, train loss = 0.2666\n",
      "Iter 180, train loss = 0.2611\n",
      "Iter 190, train loss = 0.2580\n",
      "Iter 200, train loss = 0.1683\n",
      "Iter 210, train loss = 0.5198\n",
      "Iter 220, train loss = 0.1894\n",
      "Iter 230, train loss = 0.1704\n",
      "Iter 240, train loss = 0.2092\n",
      "Iter 250, train loss = 0.1728\n",
      "Iter 260, train loss = 0.4111\n",
      "Iter 270, train loss = 0.2625\n",
      "Iter 280, train loss = 0.0715\n",
      "Iter 290, train loss = 0.2441\n",
      "Iter 300, train loss = 0.2814\n",
      "Iter 310, train loss = 0.1982\n",
      "Iter 320, train loss = 0.3734\n",
      "Iter 330, train loss = 0.3265\n",
      "Iter 340, train loss = 0.4218\n",
      "Iter 350, train loss = 0.1994\n",
      "Iter 360, train loss = 0.1522\n",
      "Iter 370, train loss = 0.3774\n",
      "Iter 10, train loss = 0.1342\n",
      "Iter 20, train loss = 0.2794\n",
      "Iter 30, train loss = 0.1981\n",
      "Iter 40, train loss = 0.1499\n",
      "Iter 50, train loss = 0.2711\n",
      "Iter 60, train loss = 0.1335\n",
      "Iter 70, train loss = 0.1176\n",
      "Iter 80, train loss = 0.2900\n",
      "Iter 90, train loss = 0.3897\n",
      "Iter 100, train loss = 0.0838\n",
      "Iter 110, train loss = 0.3789\n",
      "Iter 120, train loss = 0.1901\n",
      "Iter 130, train loss = 0.1590\n",
      "Iter 140, train loss = 0.1926\n",
      "Iter 150, train loss = 0.2353\n",
      "Iter 160, train loss = 0.2703\n",
      "Iter 170, train loss = 0.2548\n",
      "Iter 180, train loss = 0.1996\n",
      "Iter 190, train loss = 0.2068\n",
      "Iter 200, train loss = 0.1476\n",
      "Iter 210, train loss = 0.3902\n",
      "Iter 220, train loss = 0.1037\n",
      "Iter 230, train loss = 0.1178\n",
      "Iter 240, train loss = 0.1727\n",
      "Iter 250, train loss = 0.1616\n",
      "Iter 260, train loss = 0.3682\n",
      "Iter 270, train loss = 0.2620\n",
      "Iter 280, train loss = 0.0526\n",
      "Iter 290, train loss = 0.2137\n",
      "Iter 300, train loss = 0.2834\n",
      "Iter 310, train loss = 0.1644\n",
      "Iter 320, train loss = 0.4135\n",
      "Iter 330, train loss = 0.4602\n",
      "Iter 340, train loss = 0.6666\n",
      "Iter 350, train loss = 0.1915\n",
      "Iter 360, train loss = 0.1630\n",
      "Iter 370, train loss = 0.3282\n",
      "Iter 10, train loss = 0.1266\n",
      "Iter 20, train loss = 0.2519\n",
      "Iter 30, train loss = 0.1978\n",
      "Iter 40, train loss = 0.1517\n",
      "Iter 50, train loss = 0.2943\n",
      "Iter 60, train loss = 0.1512\n",
      "Iter 70, train loss = 0.1623\n",
      "Iter 80, train loss = 0.3116\n",
      "Iter 90, train loss = 0.3169\n",
      "Iter 100, train loss = 0.0735\n",
      "Iter 110, train loss = 0.3307\n",
      "Iter 120, train loss = 0.1370\n",
      "Iter 130, train loss = 0.1162\n",
      "Iter 140, train loss = 0.1579\n",
      "Iter 150, train loss = 0.1924\n",
      "Iter 160, train loss = 0.1463\n",
      "Iter 170, train loss = 0.2024\n",
      "Iter 180, train loss = 0.0999\n",
      "Iter 190, train loss = 0.1198\n",
      "Iter 200, train loss = 0.2370\n",
      "Iter 210, train loss = 0.2448\n",
      "Iter 220, train loss = 0.1107\n",
      "Iter 230, train loss = 0.0810\n",
      "Iter 240, train loss = 0.1770\n",
      "Iter 250, train loss = 0.1416\n",
      "Iter 260, train loss = 0.3637\n",
      "Iter 270, train loss = 0.1759\n",
      "Iter 280, train loss = 0.0470\n",
      "Iter 290, train loss = 0.1693\n",
      "Iter 300, train loss = 0.2876\n",
      "Iter 310, train loss = 0.1216\n",
      "Iter 320, train loss = 0.3451\n",
      "Iter 330, train loss = 0.4558\n",
      "Iter 340, train loss = 0.7408\n",
      "Iter 350, train loss = 0.1725\n",
      "Iter 360, train loss = 0.1566\n",
      "Iter 370, train loss = 0.2430\n",
      "Iter 10, train loss = 0.1238\n",
      "Iter 20, train loss = 0.1960\n",
      "Iter 30, train loss = 0.2676\n",
      "Iter 40, train loss = 0.0926\n",
      "Iter 50, train loss = 0.2681\n",
      "Iter 60, train loss = 0.1025\n",
      "Iter 70, train loss = 0.1093\n",
      "Iter 80, train loss = 0.2041\n",
      "Iter 90, train loss = 0.3594\n",
      "Iter 100, train loss = 0.0455\n",
      "Iter 110, train loss = 0.3677\n",
      "Iter 120, train loss = 0.1389\n",
      "Iter 130, train loss = 0.1318\n",
      "Iter 140, train loss = 0.1521\n",
      "Iter 150, train loss = 0.2194\n",
      "Iter 160, train loss = 0.1040\n",
      "Iter 170, train loss = 0.1903\n",
      "Iter 180, train loss = 0.0662\n",
      "Iter 190, train loss = 0.1014\n",
      "Iter 200, train loss = 0.1641\n",
      "Iter 210, train loss = 0.2754\n",
      "Iter 220, train loss = 0.0900\n",
      "Iter 230, train loss = 0.0538\n",
      "Iter 240, train loss = 0.1042\n",
      "Iter 250, train loss = 0.1320\n",
      "Iter 260, train loss = 0.2928\n",
      "Iter 270, train loss = 0.1526\n",
      "Iter 280, train loss = 0.0460\n",
      "Iter 290, train loss = 0.1520\n",
      "Iter 300, train loss = 0.2725\n",
      "Iter 310, train loss = 0.1118\n",
      "Iter 320, train loss = 0.2807\n",
      "Iter 330, train loss = 0.4077\n",
      "Iter 340, train loss = 0.7410\n",
      "Iter 350, train loss = 0.1754\n",
      "Iter 360, train loss = 0.1783\n",
      "Iter 370, train loss = 0.2072\n",
      "Iter 10, train loss = 0.1313\n",
      "Iter 20, train loss = 0.2023\n",
      "Iter 30, train loss = 0.2553\n",
      "Iter 40, train loss = 0.0632\n",
      "Iter 50, train loss = 0.2611\n",
      "Iter 60, train loss = 0.0845\n",
      "Iter 70, train loss = 0.1011\n",
      "Iter 80, train loss = 0.2067\n",
      "Iter 90, train loss = 0.3002\n",
      "Iter 100, train loss = 0.0559\n",
      "Iter 110, train loss = 0.3598\n",
      "Iter 120, train loss = 0.1534\n",
      "Iter 130, train loss = 0.1610\n",
      "Iter 140, train loss = 0.1782\n",
      "Iter 150, train loss = 0.1925\n",
      "Iter 160, train loss = 0.0559\n",
      "Iter 170, train loss = 0.1901\n",
      "Iter 180, train loss = 0.0922\n",
      "Iter 190, train loss = 0.0809\n",
      "Iter 200, train loss = 0.1211\n",
      "Iter 210, train loss = 0.2303\n",
      "Iter 220, train loss = 0.1132\n",
      "Iter 230, train loss = 0.0566\n",
      "Iter 240, train loss = 0.1266\n",
      "Iter 250, train loss = 0.1338\n",
      "Iter 260, train loss = 0.2739\n",
      "Iter 270, train loss = 0.1626\n",
      "Iter 280, train loss = 0.0304\n",
      "Iter 290, train loss = 0.1408\n",
      "Iter 300, train loss = 0.3027\n",
      "Iter 310, train loss = 0.1108\n",
      "Iter 320, train loss = 0.2253\n",
      "Iter 330, train loss = 0.4341\n",
      "Iter 340, train loss = 0.5933\n",
      "Iter 350, train loss = 0.1388\n",
      "Iter 360, train loss = 0.1195\n",
      "Iter 370, train loss = 0.2532\n",
      "Iter 10, train loss = 0.1297\n",
      "Iter 20, train loss = 0.1753\n",
      "Iter 30, train loss = 0.2617\n",
      "Iter 40, train loss = 0.0311\n",
      "Iter 50, train loss = 0.1955\n",
      "Iter 60, train loss = 0.0862\n",
      "Iter 70, train loss = 0.1157\n",
      "Iter 80, train loss = 0.2159\n",
      "Iter 90, train loss = 0.2740\n",
      "Iter 100, train loss = 0.0543\n",
      "Iter 110, train loss = 0.2882\n",
      "Iter 120, train loss = 0.1219\n",
      "Iter 130, train loss = 0.0851\n",
      "Iter 140, train loss = 0.1027\n",
      "Iter 150, train loss = 0.1575\n",
      "Iter 160, train loss = 0.0394\n",
      "Iter 170, train loss = 0.1500\n",
      "Iter 180, train loss = 0.0386\n",
      "Iter 190, train loss = 0.0960\n",
      "Iter 200, train loss = 0.0915\n",
      "Iter 210, train loss = 0.1464\n",
      "Iter 220, train loss = 0.0836\n",
      "Iter 230, train loss = 0.0865\n",
      "Iter 240, train loss = 0.1313\n",
      "Iter 250, train loss = 0.1808\n",
      "Iter 260, train loss = 0.3217\n",
      "Iter 270, train loss = 0.1358\n",
      "Iter 280, train loss = 0.0340\n",
      "Iter 290, train loss = 0.1384\n",
      "Iter 300, train loss = 0.2329\n",
      "Iter 310, train loss = 0.0983\n",
      "Iter 320, train loss = 0.2234\n",
      "Iter 330, train loss = 0.3430\n",
      "Iter 340, train loss = 0.4056\n",
      "Iter 350, train loss = 0.1438\n",
      "Iter 360, train loss = 0.0904\n",
      "Iter 370, train loss = 0.1962\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "#optimizer = optim.SGD(model.parameters(),\n",
    " #                     lr=learning_rate,\n",
    "  #                    momentum=0.9,\n",
    "   #                   nesterov=True)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                      lr=learning_rate)\n",
    "\n",
    "loss_history, val_history = train(model, optimizer)\n",
    "torch.save(model, utils.model_dir + \"conv2d_v2_std_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-99511ebbf55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history, c=\"blue\", label=\"train\")\n",
    "plt.plot(val_history, c=\"orange\", label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3693 / 4507 correct (81.94)\n",
      "Test loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(utils.test_dir, batch_size=24, transfer=False)\n",
    "model.eval()\n",
    "\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "test_loss = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, (fx, x, y) in enumerate(test_loader):\n",
    "        total += 1\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        scores = model(x)\n",
    "        _, preds = scores.max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "        test_loss += F.cross_entropy(scores, y)\n",
    "\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Test accuracy: %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    print('Test loss: {0:0.4f}'.format(val_loss / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
