{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# personal imports\n",
    "from dataloader import DataLoader\n",
    "import utils\n",
    "from utils import calculate_auc, auc\n",
    "from callbacks import *\n",
    "\n",
    "# python stuffs\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "import skimage.io\n",
    "import skimage\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from transfer_classifier import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available!\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu available!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"gpu NOT available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Numpy tensors exist in /scratch/users/gmachi/codex/data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin sanity checks for shapes...\n",
      "\n",
      "train <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n",
      "val <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n",
      "test <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n"
     ]
    }
   ],
   "source": [
    "ppb = 1 # \"patches-per-batch\"; batch size to see all 25 slices in a patch\n",
    "\n",
    "train_loader = DataLoader(utils.train_dir, batch_size=ppb, transfer=True)\n",
    "val_loader = DataLoader(utils.val_dir, batch_size=ppb, transfer=True)\n",
    "test_loader = DataLoader(utils.test_dir, batch_size=ppb, transfer=True)\n",
    "\n",
    "print(\"begin sanity checks for shapes...\\n\")\n",
    "for f, d, l in train_loader: # filename, batched data, label\n",
    "    print(\"train <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n",
    "    \n",
    "for f, d, l in val_loader:\n",
    "    print(\"val <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n",
    "    \n",
    "for f, d, l in test_loader:\n",
    "    print(\"test <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After augmentation/up-sampling, we have...\n",
      "------------------------------------------\n",
      "train set size: 13069\n",
      "val set size: 2625\n",
      "test set size: 2646\n",
      "\n",
      "See composition of patients in sets...\n",
      "--------------------------------------\n",
      "train set unique files: {'034', '024', '027', '012', '008', '020', '007', '014', '015', '004'}\n",
      "val set unique files: {'011', '016', '030', '023'}\n",
      "test set unique files: {'005', '017', '006', '019'}\n",
      "\n",
      "(+/-) splits in sets...\n",
      "-----------------------\n",
      "train set split: (6897, 6172)\n",
      "val set split: (1750, 875)\n",
      "test set split: (1764, 882)\n"
     ]
    }
   ],
   "source": [
    "# Get image summary stats\n",
    "\n",
    "from utils import labels_dict\n",
    "\n",
    "def count_files(dir):\n",
    "    return len([1 for x in list(os.scandir(dir)) if x.is_file()])\n",
    "\n",
    "def unique_files(dir):\n",
    "    return set([x.split(\"_\")[0].split(\"reg\")[1] for x in os.listdir(dir)])\n",
    "\n",
    "def set_splits(dir):\n",
    "    all_files = [x.split(\"_\")[0].split(\"reg\")[1] for x in os.listdir(dir)]\n",
    "    labels = [labels_dict[u][1] for u in all_files]\n",
    "    pos = np.sum(labels)\n",
    "    neg = len(labels) - pos\n",
    "    return pos, neg\n",
    "    \n",
    "\n",
    "print(\"After augmentation/up-sampling, we have...\\n------------------------------------------\")\n",
    "print(\"train set size:\", count_files(utils.train_dir))\n",
    "print(\"val set size:\", count_files(utils.val_dir))\n",
    "print(\"test set size:\", count_files(utils.test_dir))\n",
    "\n",
    "print(\"\\nSee composition of patients in sets...\\n--------------------------------------\")\n",
    "print(\"train set unique files:\", unique_files(utils.train_dir))\n",
    "print(\"val set unique files:\", unique_files(utils.val_dir))\n",
    "print(\"test set unique files:\", unique_files(utils.test_dir))\n",
    "\n",
    "print(\"\\n(+/-) splits in sets...\\n-----------------------\")\n",
    "print(\"train set split:\", set_splits(utils.train_dir))\n",
    "print(\"val set split:\", set_splits(utils.val_dir))\n",
    "print(\"test set split:\", set_splits(utils.test_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition - VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg19(pretrained=True).features\n",
    "print(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning: apply pre-trained weights to 96x96x3 patch-slices \n",
    "This process gives us slice-level tensors for aggregation for the whole patch (96x96x75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "    \n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = nn.Sequential(\n",
    "    Flatten(),\n",
    "    \n",
    "    nn.Linear(in_features= 4608, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    \n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace = True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    \n",
    "    nn.Linear(in_features=4096, out_features=2, bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_t, val_loader):\n",
    "    it = 0\n",
    "    pooled_val_data = []\n",
    "    pooled_val_labels =[]\n",
    "    val_losses = []\n",
    "    for fdl_val in val_loader:\n",
    "        \n",
    "        (f_val, d_val, l_val) = fdl_val\n",
    "\n",
    "        d_val = torch.from_numpy(d_val)\n",
    "        l_val = torch.from_numpy(l_val)\n",
    "\n",
    "        d_val = d_val.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        l_val = l_val.to(device=device, dtype=torch.long)\n",
    "\n",
    "        d_slice_v = model(d_val)\n",
    "        d_pooled_v = pool_batch(d_slice_v, mode='mean')\n",
    "        l_pooled_v = pool_labels(l_val)\n",
    "        pooled_val_data.append(d_pooled_v)\n",
    "        pooled_val_labels.append(l_pooled_v)\n",
    "        \n",
    "        if it != 0 and it % 4:\n",
    "            d_pooled_v = torch.cat(pooled_val_data, dim=0)\n",
    "            l_pooled_v = torch.cat(pooled_val_labels, dim=0)\n",
    "            pooled_val_data = []\n",
    "            pooled_val_labels =[]\n",
    "            \n",
    "            val_scores = model_t(d_pooled_v)\n",
    "            val_loss = F.cross_entropy(val_scores, l_pooled_v)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "        it += 0\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(model, model_t, vgg_optimizer, t_optimizer):\n",
    "    ppb = 5\n",
    "    print_every = 10\n",
    "    epochs = 10\n",
    "\n",
    "    train_loader = DataLoader(utils.train_dir, batch_size=ppb, transfer=True)\n",
    "\n",
    "    i = 0 # batch number\n",
    "    train_losses, val_losses = [], []\n",
    "    cur_val = 999\n",
    "    consec_increases = 0\n",
    "    model = model.to(device=device)\n",
    "    model_t = model_t.to(device=device)\n",
    "    \n",
    "    model.eval()\n",
    "#     model.train()\n",
    "    model_t.train()\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        pooled_train_data = []\n",
    "        pooled_train_labels =[]\n",
    "        ct = 0\n",
    "        for fdl_train in train_loader:\n",
    "\n",
    "            # train\n",
    "            (f_train, d_train, l_train) = fdl_train\n",
    "            \n",
    "            d_train = torch.from_numpy(d_train)\n",
    "            l_train = torch.from_numpy(l_train)\n",
    "            \n",
    "            d_train = d_train.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            l_train = l_train.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            d_slice_t = model(d_train) # (5*25, 3, 3, 512)\n",
    "            d_pooled_t = pool_batch(d_slice_t, batch_size=ppb, mode='max') # (5, 3, 3, 512)\n",
    "            l_pooled_t = pool_labels(l_train)\n",
    "            pooled_train_data.append(d_pooled_t)\n",
    "            pooled_train_labels.append(l_pooled_t)\n",
    "            \n",
    "            if i != 0 and i % 2 == 0:\n",
    "                \n",
    "                d_pooled_t = torch.cat(pooled_train_data, dim=0)\n",
    "                l_pooled_t = torch.cat(pooled_train_labels, dim=0)\n",
    "\n",
    "                pooled_train_data = []\n",
    "                pooled_train_labels =[]\n",
    "            \n",
    "                # get train metrics\n",
    "                train_scores = model_t(d_pooled_t)\n",
    "                train_loss = F.cross_entropy(train_scores, l_pooled_t)\n",
    "\n",
    "                vgg_optimizer.zero_grad()\n",
    "                t_optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                vgg_optimizer.step()\n",
    "                t_optimizer.step()\n",
    "                \n",
    "                print(d_train.grad)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                if ct % print_every == 0:\n",
    "                    print(\"iter:\", ct+print_every, \"train:\", train_loss.item())\n",
    "\n",
    "\n",
    "                # update cur_val\n",
    "                ct += 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        torch.save(model, utils.model_dir + \"transfer_epoch%s_maxpool_vgg.pt\" % e)\n",
    "        torch.save(model_t, utils.model_dir + \"transfer_epoch%s_maxpool_clf.pt\" % e)\n",
    "#         with torch.no_grad():\n",
    "#             model_t.eval()\n",
    "#             print('validating...')\n",
    "#             val_loader = DataLoader(utils.val_dir, batch_size=ppb, transfer=True)\n",
    "#             val_loss = validation(model_t, val_loader)\n",
    "#             val_losses.append(val_loss)\n",
    "        \n",
    "        print('\\nepoch:', e, 'train:', train_loss.item()) #, \"val:\", val_loss)\n",
    "        \n",
    "        \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "iter: 10 train: 0.6485349535942078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0470d9482470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                       lr=learning_rate)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"transfer_full_maxpool_vgg.pt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"transfer_full_maxpool_clf.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1b261ef5ecd2>\u001b[0m in \u001b[0;36mtrain_transfer\u001b[0;34m(model, model_t, vgg_optimizer, t_optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0md_slice_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (5*25, 3, 3, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0md_pooled_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_slice_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (5, 3, 3, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0ml_pooled_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mpooled_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_pooled_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mpooled_train_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_pooled_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codex/code/transfer_classifier.py\u001b[0m in \u001b[0;36mpool_labels\u001b[0;34m(labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpool_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "#optimizer = optim.SGD(model.parameters(),\n",
    " #                     lr=learning_rate,\n",
    "  #                    momentum=0.9,\n",
    "   #                   nesterov=True)\n",
    "    \n",
    "vgg_optimizer = optim.Adam(model.parameters(),\n",
    "                      lr=learning_rate)\n",
    "t_optimizer = optim.Adam(model_t.parameters(),\n",
    "                      lr=learning_rate)\n",
    "\n",
    "loss_history, val_history = train_transfer(model, model_t, vgg_optimizer, t_optimizer)\n",
    "torch.save(model, utils.model_dir + \"transfer_full_maxpool_vgg.pt\" % e)\n",
    "torch.save(model_t, utils.model_dir + \"transfer_full_maxpool_clf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # cute printout for sanity (~27,000 train)\n",
    "#     if (i > 0) and ((i+1) % print_every == 0):\n",
    "#         print(\"%i patches complete\" % ((i+1)*ppb))\n",
    "\n",
    "\n",
    "# train_names = ['train_loss', 'train_acc', \"train_auc\"]\n",
    "# val_names = ['val_loss', 'val_acc', \"val_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
