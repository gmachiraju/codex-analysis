{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11008254872900864376\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1109531646718909695\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# personal imports\n",
    "from dataloader import DataLoader\n",
    "import utils\n",
    "from utils import calculate_auc, auc\n",
    "from callbacks import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python stuffs\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "import skimage.io\n",
    "import skimage\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, Flatten, Concatenate \n",
    "# Conv2D, Input, Flatten, MaxPooling2D, UpSampling2D, concatenate, Cropping2D, Reshape, BatchNormalization\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Numpy tensors exist in /scratch/users/gmachi/codex/data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin sanity checks for shapes...\n",
      "\n",
      "train <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n",
      "val <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n",
      "test <filenames, data batch, labels>:\n",
      " 25 (25, 3, 96, 96) (25,)\n"
     ]
    }
   ],
   "source": [
    "ppb = 1 # \"patches-per-batch\"; batch size to see all 25 slices in a patch\n",
    "\n",
    "train_loader = DataLoader(utils.train_dir, batch_size=ppb, transfer=True)\n",
    "val_loader = DataLoader(utils.val_dir, batch_size=ppb, transfer=True)\n",
    "test_loader = DataLoader(utils.test_dir, batch_size=ppb, transfer=True)\n",
    "\n",
    "print(\"begin sanity checks for shapes...\\n\")\n",
    "for f, d, l in train_loader: # filename, batched data, label\n",
    "    print(\"train <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n",
    "    \n",
    "for f, d, l in val_loader:\n",
    "    print(\"val <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n",
    "    \n",
    "for f, d, l in test_loader:\n",
    "    print(\"test <filenames, data batch, labels>:\\n\", len(f), d.shape, l.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After augmentation/up-sampling, we have...\n",
      "------------------------------------------\n",
      "train set size: 13069\n",
      "val set size: 4449\n",
      "test set size: 4507\n",
      "\n",
      "See composition of patients in sets...\n",
      "--------------------------------------\n",
      "train set unique files: {'007', '024', '014', '004', '015', '012', '008', '034', '027', '020'}\n",
      "val set unique files: {'016', '011', '030', '023'}\n",
      "test set unique files: {'005', '017', '019', '006'}\n",
      "\n",
      "(+/-) splits in sets...\n",
      "-----------------------\n",
      "train set split: (6897, 6172)\n",
      "val set split: (3574, 875)\n",
      "test set split: (3625, 882)\n"
     ]
    }
   ],
   "source": [
    "# Get image summary stats\n",
    "\n",
    "from utils import labels_dict\n",
    "\n",
    "def count_files(dir):\n",
    "    return len([1 for x in list(os.scandir(dir)) if x.is_file()])\n",
    "\n",
    "def unique_files(dir):\n",
    "    return set([x.split(\"_\")[0].split(\"reg\")[1] for x in os.listdir(dir)])\n",
    "\n",
    "def set_splits(dir):\n",
    "    all_files = [x.split(\"_\")[0].split(\"reg\")[1] for x in os.listdir(dir)]\n",
    "    labels = [labels_dict[u][1] for u in all_files]\n",
    "    pos = np.sum(labels)\n",
    "    neg = len(labels) - pos\n",
    "    return pos, neg\n",
    "    \n",
    "\n",
    "print(\"After augmentation/up-sampling, we have...\\n------------------------------------------\")\n",
    "print(\"train set size:\", count_files(utils.train_dir))\n",
    "print(\"val set size:\", count_files(utils.val_dir))\n",
    "print(\"test set size:\", count_files(utils.test_dir))\n",
    "\n",
    "print(\"\\nSee composition of patients in sets...\\n--------------------------------------\")\n",
    "print(\"train set unique files:\", unique_files(utils.train_dir))\n",
    "print(\"val set unique files:\", unique_files(utils.val_dir))\n",
    "print(\"test set unique files:\", unique_files(utils.test_dir))\n",
    "\n",
    "print(\"\\n(+/-) splits in sets...\\n-----------------------\")\n",
    "print(\"train set split:\", set_splits(utils.train_dir))\n",
    "print(\"val set split:\", set_splits(utils.val_dir))\n",
    "print(\"test set split:\", set_splits(utils.test_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition - VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu NOT avilable\n"
     ]
    }
   ],
   "source": [
    "# use appropriate device\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"gpu avilable\")\n",
    "    with K.tf.device('/gpu:1'):\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "               inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "               device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "        session = tf.Session(config=config)\n",
    "        K.set_session(session)\n",
    "        \n",
    "        # sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "else:\n",
    "    print(\"gpu NOT avilable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10048019050400888493\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6639925133493565300\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out layer shape: (?, 3, 3, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 96, 96, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 96, 96, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 48, 48, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 48, 48, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 24, 24, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "x = vgg19.output\n",
    "print(\"out layer shape:\", x.shape)\n",
    "\n",
    "model = Model(inputs=vgg19.input, outputs=x) \n",
    "# the outputs are sent to Alex for pooling and FCC\n",
    "model.summary()\n",
    "\n",
    "# Train only the top layer\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.000005)\n",
    "optimizer = Adam(lr=0.001)\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['acc', auc]\n",
    "model.compile(optimizer, loss, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning: apply pre-trained weights to 96x96x3 patch-slices \n",
    "This process gives us slice-level tensors for aggregation for the whole patch (96x96x75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# from dataloader import TransferLoader\n",
    "from transfer_classifier import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_log(callback, names, logs, batch_no):\n",
    "#     for name, value in zip(names, logs):\n",
    "#         summary = tf.Summary()\n",
    "#         summary_value = summary.value.add()\n",
    "#         summary_value.simple_value = value\n",
    "#         summary_value.tag = name\n",
    "#         callback.writer.add_summary(summary, batch_no)\n",
    "#         callback.writer.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3, 3, 512) --> (3, 3, 512)\n"
     ]
    }
   ],
   "source": [
    "ppb = 1             # patches per batch\n",
    "N,H,W,C = x.shape   # VGG shape\n",
    "N = ppb*25          # batch size\n",
    "\n",
    "dummy_tensor = np.zeros((N,H,W,C))\n",
    "pooled = pool(dummy_tensor)\n",
    "print(dummy_tensor.shape, \"-->\", pooled.shape)\n",
    "\n",
    "# TL model instantiation\n",
    "#------------------------\n",
    "model_t = classify_from_pooled(pooled.shape)\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, min_lr=0.000005)\n",
    "\n",
    "learning_rate = 1e-5 \n",
    "optimizer = Adam(lr=learning_rate)\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['acc', auc]\n",
    "model_t.compile(optimizer, loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "# file_dir = './TransferModel'\n",
    "# if not os.path.exists(file_dir):\n",
    "#     os.mkdir(file_dir)\n",
    "    \n",
    "# model_name = 'vgg19_transfer'\n",
    "# network_filepath = os.path.join(file_dir, model_name + '.h5')\n",
    "\n",
    "# callback = TensorBoard(file_dir)\n",
    "# callback.set_model(model_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(model, model_t):\n",
    "    ppb = 1\n",
    "    print_every = 10\n",
    "    epochs = 10\n",
    "\n",
    "    train_loader = DataLoader(utils.data_dir + 'train/', batch_size=ppb, transfer=True)\n",
    "    val_loader = DataLoader(utils.data_dir + 'val/', batch_size=ppb, transfer=True)\n",
    "\n",
    "    i = 0 # batch number\n",
    "    train_losses, val_losses = [], []\n",
    "    cur_val = 999\n",
    "    consec_increases = 0\n",
    "\n",
    "    print(\"printing: loss, accuracy, AUC:\\n-----------------------------\")\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for fdl_train, fdl_val in zip(train_loader, val_loader):\n",
    "\n",
    "            # train\n",
    "            (f_train, d_train, l_train) = fdl_train\n",
    "            d_slice_t = model.predict_on_batch(x=d_train.transpose(0,2,3,1))\n",
    "            d_pooled_t = pool(d_slice_t)\n",
    "            d_pooled_t = d_pooled_t[np.newaxis, :, :, :]\n",
    "            l_pooled_t = np.array([l_train[0]])[np.newaxis, :] # all should be the same\n",
    "\n",
    "            # val\n",
    "            (f_val, d_val, l_val) = fdl_val\n",
    "            d_slice_v = model.predict_on_batch(x=d_val.transpose(0,2,3,1))\n",
    "            d_pooled_v = pool(d_slice_v)\n",
    "            d_pooled_v = d_pooled_v[np.newaxis, :, :, :]\n",
    "            l_pooled_v = np.array([l_val[0]])[np.newaxis, :] # all should be the same\n",
    "\n",
    "            # get train metrics\n",
    "            train_loss = model_t.train_on_batch(d_pooled_t, l_pooled_t)\n",
    "            train_losses.append(train_loss)\n",
    "        #     write_log(callback, train_names, logs, i)\n",
    "\n",
    "            # get val metrics\n",
    "            if i % 10 == 0:\n",
    "                val_loss = model_t.test_on_batch(d_pooled_v, l_pooled_v)\n",
    "                val_losses.append(val_loss)\n",
    "                print(\"iter:\", (i+1)*10, \"train:\", train_loss, \"val:\", val_loss)\n",
    "        #         write_log(callback, val_names, logs, i)\n",
    "\n",
    "            # early stopping criteria\n",
    "            if val_loss[0] > cur_val:\n",
    "                consec_increases += 1\n",
    "            else:\n",
    "                consec_increases = 0\n",
    "            \n",
    "            # update cur_val\n",
    "            cur_val = val_loss[0]\n",
    "\n",
    "            if consec_increases >= 3:\n",
    "                print('Stopping early due to validation loss increase')\n",
    "                model_t.save(utils.model_dir + \"transfer_earlystop_epoch{}.h5\".format(e))\n",
    "                return train_losses, val_losses\n",
    "\n",
    "            model_t.save(utils.model_dir + \"transfer_epoch%s.h5\" % e)\n",
    "            i += 1\n",
    "        \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing: loss, accuracy, AUC:\n",
      "-----------------------------\n",
      "iter: 0 train: [15.942385, 0.0, 0.3958333] val: [16.118095, 0.0, 0.3611111]\n",
      "iter: 10 train: [1.1920933e-07, 1.0, 0.43333328] val: [1.1920933e-07, 1.0, 0.44374996]\n",
      "iter: 20 train: [1.1920933e-07, 1.0, 0.5064935] val: [1.8232073e-05, 1.0, 0.530303]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-29d92d55e803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-d6dd37a4c913>\u001b[0m in \u001b[0;36mtrain_transfer\u001b[0;34m(model, model_t)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdl_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0md_slice_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0md_pooled_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_slice_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0md_pooled_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_pooled_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_transfer(model, model_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # cute printout for sanity (~27,000 train)\n",
    "#     if (i > 0) and ((i+1) % print_every == 0):\n",
    "#         print(\"%i patches complete\" % ((i+1)*ppb))\n",
    "\n",
    "\n",
    "# train_names = ['train_loss', 'train_acc', \"train_auc\"]\n",
    "# val_names = ['val_loss', 'val_acc', \"val_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
